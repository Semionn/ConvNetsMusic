{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 660 (CNMeM is disabled, cuDNN 4007)\n",
      "/home/semionn/anaconda2/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import librosa as lb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().magic(u'matplotlib inline')\n",
    "\n",
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "# Seed for reproduciblity\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "# get_ipython().system(u'wget -N http://deeplearning.net/data/mnist/mnist.pkl.gz')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "wav_dir = '/media/semionn/\\xd0\\xa5\\xd0\\xb0\\xd1\\x80\\xd0\\xb4/music_dataset_wav'\n",
    "X = []\n",
    "Y = []\n",
    "clazzes = {}\n",
    "for root, dirs, files in os.walk(wav_dir):\n",
    "    X += list(map(lambda x: os.path.join(root, x), files))\n",
    "    clazz = clazzes[root] if (root in clazzes) else len(clazzes)\n",
    "    clazzes[root] = clazz\n",
    "    for _ in files:\n",
    "        Y.append(clazz-1)\n",
    "    \n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train = X\n",
    "y_train = Y\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1)\n",
    "y_val = y_train\n",
    "X_val = X_train\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.11)\n",
    "# print(set(y_train))\n",
    "\n",
    "# print(X_train)\n",
    "print(len(clazzes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "import functools\n",
    "\n",
    "def memoize(obj):\n",
    "    cache = obj.cache = {}\n",
    "\n",
    "    @functools.wraps(obj)\n",
    "    def memoizer(*args, **kwargs):\n",
    "        key = str(args) + str(kwargs)\n",
    "        if key not in cache:\n",
    "            cache[key] = obj(*args, **kwargs)\n",
    "        return cache[key]\n",
    "\n",
    "    return memoizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import librosa as lb\n",
    "def f(x):\n",
    "    return x.real\n",
    "\n",
    "# f = np.vectorize(f)\n",
    "\n",
    "@memoize\n",
    "def get_file_spectre(filename):\n",
    "    src, sr = lb.load (filename, sr=11025)\n",
    "    return src, sr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_sample(filename, n_batch=10):\n",
    "    src, sr = get_file_spectre(filename)\n",
    "    frame_size = sr*4\n",
    "    res = []\n",
    "    idx = np.random.choice(len(src) - frame_size, n_batch)\n",
    "    for pos in idx:\n",
    "        frame = src[pos: pos + frame_size]\n",
    "        SRC = lb.stft (frame, n_fft=512, hop_length=256)\n",
    "        res.append(f(SRC))\n",
    "    return res\n",
    "\n",
    "def batch_gen(X, y, n_batch):\n",
    "    while True:\n",
    "        idx = randint(0, len(X)-1)\n",
    "        xs = []\n",
    "        ys = []\n",
    "        for x in gen_sample(X[idx], n_batch):\n",
    "            xs.append(x)\n",
    "            ys.append(y[idx])\n",
    "        yield np.array(xs), np.array(ys).astype('int32')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1, 257, 173)\n",
      "(None, 16, 257, 1)\n",
      "(None, 16, 128, 1)\n",
      "(None, 16, 128, 1)\n",
      "(None, 16, 128, 3)\n",
      "l_conv2_36 (None, 16, 128, 1)\n",
      "l_conv2_1 (None, 16, 128, 1)\n",
      "concat_1 (None, 32, 128, 1)\n",
      "(None, 32, 64, 1)\n",
      "(None, 16, 62, 1)\n",
      "(None, 16, 31, 1)\n",
      "(None, 16, 29, 1)\n",
      "(None, 16, 14, 1)\n",
      "(None, 16, 12, 1)\n",
      "(None, 16, 6, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "l_in = lasagne.layers.InputLayer((None, 257, 173))\n",
    "\n",
    "l_shape = lasagne.layers.ReshapeLayer(l_in, (-1, 1, 257, 173))\n",
    "print(lasagne.layers.get_output_shape(l_shape))\n",
    "\n",
    "l_conv = lasagne.layers.Conv2DLayer(l_shape, num_filters=16, stride=1, filter_size=(1, 173), pad='valid')\n",
    "print(lasagne.layers.get_output_shape(l_conv))\n",
    "\n",
    "l_pool = lasagne.layers.MaxPool2DLayer(l_conv, pool_size=(2, 1))\n",
    "print(lasagne.layers.get_output_shape(l_pool))\n",
    "\n",
    "l_conv2_31 = lasagne.layers.Conv2DLayer(l_pool, num_filters=16, stride=1, filter_size=(1, 1), pad='valid')\n",
    "print(lasagne.layers.get_output_shape(l_conv2_31))\n",
    "\n",
    "l_conv2_33 = lasagne.layers.Conv2DLayer(l_conv2_31, num_filters=16, stride=1, filter_size=(3, 1), pad=1)\n",
    "print(lasagne.layers.get_output_shape(l_conv2_33))\n",
    "\n",
    "l_conv2_36 = lasagne.layers.Conv2DLayer(l_conv2_33, num_filters=16, stride=1, filter_size=(1, 3), pad='valid')\n",
    "print('l_conv2_36 %s' % str(lasagne.layers.get_output_shape(l_conv2_36)))\n",
    "\n",
    "l_conv2_1 = lasagne.layers.Conv2DLayer(l_pool, num_filters=16, stride=1, filter_size=(1, 1), pad='valid')\n",
    "print('l_conv2_1 %s' % str(lasagne.layers.get_output_shape(l_conv2_1)))\n",
    "\n",
    "l_concat_1 = lasagne.layers.ConcatLayer([l_conv2_36, l_conv2_1], axis=1)\n",
    "print('concat_1 %s' % str(lasagne.layers.get_output_shape(l_concat_1)))\n",
    "\n",
    "l_pool2 = lasagne.layers.MaxPool2DLayer(l_concat_1, pool_size=(2, 1))\n",
    "print(lasagne.layers.get_output_shape(l_pool2))\n",
    "\n",
    "l_conv3 = lasagne.layers.Conv2DLayer(l_pool2, num_filters=16, stride=1, filter_size=(3, 1), pad='valid')\n",
    "print(lasagne.layers.get_output_shape(l_conv3))\n",
    "\n",
    "l_pool3 = lasagne.layers.MaxPool2DLayer(l_conv3, pool_size=(2, 1))\n",
    "print(lasagne.layers.get_output_shape(l_pool3))\n",
    "\n",
    "l_conv4 = lasagne.layers.Conv2DLayer(l_pool3, num_filters=16, stride=1, filter_size=(3, 1), pad='valid')\n",
    "print(lasagne.layers.get_output_shape(l_conv4))\n",
    "\n",
    "l_pool4 = lasagne.layers.MaxPool2DLayer(l_conv4, pool_size=(2, 1))\n",
    "print(lasagne.layers.get_output_shape(l_pool4))\n",
    "\n",
    "l_conv5 = lasagne.layers.Conv2DLayer(l_pool4, num_filters=16, stride=1, filter_size=(3, 1), pad='valid')\n",
    "print(lasagne.layers.get_output_shape(l_conv5))\n",
    "\n",
    "l_pool5 = lasagne.layers.Pool2DLayer(l_conv5, pool_size=(2, 1), mode='average_inc_pad')\n",
    "print(lasagne.layers.get_output_shape(l_pool5))\n",
    "\n",
    "l_out = lasagne.layers.DenseLayer(l_pool5,\n",
    "                                  num_units=len(clazzes),\n",
    "                                  nonlinearity=lasagne.nonlinearities.softmax)\n",
    "layers = ['l_conv', 'l_pool', 'l_conv2_31', 'l_conv2_33', 'l_conv2_36', 'l_conv2_1', 'l_concat_1', 'l_pool2', 'l_conv3', 'l_pool3', 'l_conv4', 'l_pool4', 'l_conv5', 'l_pool5', 'l_out']\n",
    "net = {}\n",
    "net['l_conv'] = l_conv\n",
    "net['l_pool'] = l_pool\n",
    "net['l_conv2_31'] = l_conv2_31\n",
    "net['l_conv2_33'] = l_conv2_33\n",
    "net['l_conv2_36'] = l_conv2_36\n",
    "net['l_conv2_1'] = l_conv2_1\n",
    "net['l_concat_1'] = l_concat_1\n",
    "net['l_pool2'] = l_pool2\n",
    "net['l_conv3'] = l_conv3\n",
    "net['l_pool3'] = l_pool3\n",
    "net['l_conv4'] = l_conv4\n",
    "net['l_pool4'] = l_pool4\n",
    "net['l_conv5'] = l_conv5\n",
    "net['l_pool5'] = l_pool5\n",
    "net['l_out'] = l_out\n",
    "\n",
    "layers = {k: net[k] for k in layers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def get_samples(filename, offset=0):\n",
    "#     src, sr = lb.load (filename, sr=11025)\n",
    "#     frame_size = sr*4\n",
    "#     res = []\n",
    "#     for pos in range(len(src)//2, len(src), frame_size):\n",
    "#         frame = src[pos: pos + frame_size]\n",
    "#         SRC = lb.stft (frame, n_fft=512, hop_length=256)\n",
    "#         res.append(f(SRC))\n",
    "#         break\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compile and train the network.\n",
    "# Accuracy is much better than the single layer network, despite the small number of filters.\n",
    "X_sym = T.tensor3()\n",
    "y_sym = T.ivector()\n",
    "\n",
    "output = lasagne.layers.get_output(l_out, X_sym)\n",
    "pred = output.argmax(-1)\n",
    "\n",
    "loss = T.mean(lasagne.objectives.categorical_crossentropy(output, y_sym))\n",
    "\n",
    "acc = T.mean(T.eq(pred, y_sym))\n",
    "\n",
    "params = lasagne.layers.get_all_params(l_out)\n",
    "grad = T.grad(loss, params)\n",
    "updates = lasagne.updates.adam(grad, params, learning_rate=0.05)\n",
    "\n",
    "f_train = theano.function([X_sym, y_sym], [loss, acc], updates=updates)\n",
    "f_val = theano.function([X_sym, y_sym], [loss, acc])\n",
    "f_predict = theano.function([X_sym], pred)\n",
    "# values = pickle.load(open('weights0.200.pkl', 'rb'))\n",
    "# lasagne.layers.set_all_param_values(layers.values(), values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train (val) loss 4.036 (4.856) ratio 1.203\n",
      "Train (val) accuracy 0.060 (0.000)\n",
      "Epoch 1, Train (val) loss 3.929 (3.660) ratio 0.932\n",
      "Train (val) accuracy 0.080 (0.000)\n",
      "Epoch 2, Train (val) loss 4.298 (4.233) ratio 0.985\n",
      "Train (val) accuracy 0.000 (0.000)\n",
      "Epoch 3, Train (val) loss 4.026 (4.348) ratio 1.080\n",
      "Train (val) accuracy 0.040 (0.100)\n",
      "Epoch 4, Train (val) loss 3.849 (4.312) ratio 1.120\n",
      "Train (val) accuracy 0.060 (0.000)\n",
      "Epoch 5, Train (val) loss 4.407 (3.887) ratio 0.882\n",
      "Train (val) accuracy 0.000 (0.000)\n",
      "Epoch 6, Train (val) loss 3.997 (4.435) ratio 1.109\n",
      "Train (val) accuracy 0.000 (0.100)\n",
      "Epoch 7, Train (val) loss 4.123 (4.530) ratio 1.099\n",
      "Train (val) accuracy 0.020 (0.100)\n",
      "Epoch 8, Train (val) loss 4.028 (4.653) ratio 1.155\n",
      "Train (val) accuracy 0.040 (0.000)\n",
      "Epoch 9, Train (val) loss 4.204 (4.563) ratio 1.086\n",
      "Train (val) accuracy 0.020 (0.000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BATCH_SIZE = 20\n",
    "N_BATCHES = 50\n",
    "N_VAL_BATCHES = 10\n",
    "EPOCH_CNT = 10\n",
    "\n",
    "train_batches = batch_gen(X_train, y_train, BATCH_SIZE)\n",
    "val_batches = batch_gen(X_val, y_val, BATCH_SIZE)\n",
    "\n",
    "for epoch in range(EPOCH_CNT):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    for i in range(N_BATCHES):\n",
    "#         print('Train batch #{}'.format(i))\n",
    "        X1, y1 = next(train_batches)\n",
    "#         print(X1.shape)\n",
    "#         print(y1.shape)\n",
    "        loss, acc = f_train(X1, y1)\n",
    "        train_loss += loss\n",
    "        train_acc += acc\n",
    "    train_loss /= N_BATCHES\n",
    "    train_acc /= N_BATCHES\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    for i in range(N_VAL_BATCHES):\n",
    "#         print('Validate batch #{}'.format(i))\n",
    "        X1, y1 = next(val_batches)\n",
    "        loss, acc = f_val(X1, y1)\n",
    "        val_loss += loss\n",
    "        val_acc += acc\n",
    "    val_loss /= N_VAL_BATCHES\n",
    "    val_acc /= N_VAL_BATCHES\n",
    "    \n",
    "    print('Epoch {}, Train (val) loss {:.03f} ({:.03f}) ratio {:.03f}'.format(\n",
    "            epoch, train_loss, val_loss, val_loss/train_loss))\n",
    "    print('Train (val) accuracy {:.03f} ({:.03f})'.format(train_acc, val_acc))\n",
    "\n",
    "    all_weights = lasagne.layers.get_all_param_values(layers.values())\n",
    "    pickle.dump(all_weights, open('weights{:.03f}.pkl'.format(val_acc), 'wb+'), pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values = pickle.load(open('weights.pkl'))['param values']\n",
    "lasagne.layers.set_all_param_values(layers, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    x = x.flatten(ndim=3)\n",
    "    g = T.tensordot(x, x, axes=([2], [2]))\n",
    "    return g\n",
    "\n",
    "\n",
    "def content_loss(P, X, layer):\n",
    "    p = P[layer]\n",
    "    x = X[layer]\n",
    "    \n",
    "    loss = 1./2 * ((x - p)**2).sum()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def style_loss(A, X, layer):\n",
    "    a = A[layer]\n",
    "    x = X[layer]\n",
    "    \n",
    "    A = gram_matrix(a)\n",
    "    G = gram_matrix(x)\n",
    "    \n",
    "    N = a.shape[1]\n",
    "    M = a.shape[2] * a.shape[3]\n",
    "    \n",
    "    loss = 1./(4 * N**2 * M**2) * ((G - A)**2).sum()\n",
    "    return loss\n",
    "\n",
    "def total_variation_loss(x):\n",
    "    return (((x[:,:-1,:-1] - x[:,1:,:-1])**2 + (x[:,:-1,:-1] - x[:,:-1,1:])**2)**1.25).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "style_samples = get_samples('converted_music/miniMusic/acrossu.mid')\n",
    "target_samples = get_samples('converted_music/miniMusic/013705b_.mid')\n",
    "style_music = style_samples[0]\n",
    "target_music = target_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_im_theano = T.matrix()\n",
    "outputs = lasagne.layers.get_output(layers.values(), input_im_theano)\n",
    "\n",
    "target_features = {k: theano.shared(output.eval({input_im_theano: target_music}))\n",
    "                  for k, output in zip(layers.keys(), outputs)}\n",
    "style_features = {k: theano.shared(output.eval({input_im_theano: style_music}))\n",
    "                for k, output in zip(layers.keys(), outputs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generated_music = theano.shared(floatX(np.random.uniform(-128, 128, (1, 257, 173))))\n",
    "\n",
    "gen_features = lasagne.layers.get_output(layers.values(), generated_music)\n",
    "gen_features = {k: v for k, v in zip(layers.keys(), gen_features)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "losses = []\n",
    "\n",
    "# content loss\n",
    "losses.append(0.001 * content_loss(target_features, gen_features, 'conv4_2'))\n",
    "\n",
    "# style loss\n",
    "losses.append(0.2e6 * style_loss(style_features, gen_features, 'conv1_1'))\n",
    "losses.append(0.2e6 * style_loss(style_features, gen_features, 'conv2_1'))\n",
    "losses.append(0.2e6 * style_loss(style_features, gen_features, 'conv3_1'))\n",
    "losses.append(0.2e6 * style_loss(style_features, gen_features, 'conv4_1'))\n",
    "losses.append(0.2e6 * style_loss(style_features, gen_features, 'conv5_1'))\n",
    "\n",
    "# total variation penalty\n",
    "losses.append(0.1e-7 * total_variation_loss(generated_music))\n",
    "\n",
    "total_loss = sum(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
